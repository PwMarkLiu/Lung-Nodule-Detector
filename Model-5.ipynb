{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Nodule Detector Model\n",
    "\n",
    "## My first pass using  UNet-like architecture [Link](https://www.researchgate.net/figure/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al_fig2_323597886)\n",
    "\n",
    "This model is trained using LUNA 16 dataset. The preprocessing procedure will be uploaded in a later date\n",
    "\n",
    "I used Deep-Learning-with-PyTorch book ( Eli Stevens and Luca Antiga) as reference, but I used different data preprocessing, data augmentation, model architecture, a learning rate schedule, Dice loss, as well as weighted BCE Loss.\n",
    "\n",
    "- Initialize model + data loader\n",
    "- data augmentation\n",
    "- visualizing data together with labeled data. (plot it out or use ipywidget)\n",
    "- Pass the batches into model\n",
    "- calculate train loss + back propagation\n",
    "- calculate validation loss + record params\n",
    "- save weights and print out all info..\n",
    "- Visualize prediction compared to labels.\n",
    "- Get the candidates of training data that the model is doing poorly against (False positive candidates) and to later train on that data to reduce false positive rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset VRAM\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use Line messenger to send me loss updates while working on my full time job!\n",
    "from parinya import LINE\n",
    "line =  LINE(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "# from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import rotate, resize\n",
    "from random import randint\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to define a dataloader https://www.kaggle.com/dhananjay3/image-segmentation-from-scratch-in-pytorch\n",
    "\n",
    "- Note to self: glob.glob is important else will not get all the files\n",
    "- Note: the torchvision augmentation doesn't support ndarray, so I tried using PIL - Image.fromarray(image) - still causing error. I decided to use skimage instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderImg(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path=\"train3\", random_rotation=None, get_path=False):\n",
    "        super(DataLoaderImg, self).__init__()\n",
    "        self.img_files = glob.glob(os.path.join(folder_path, '*.npy'))\n",
    "        self.random_rotation = random_rotation\n",
    "        self.get_path = get_path\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = np.load(img_path)\n",
    "            image = data[0]\n",
    "            label = data[1]\n",
    "            if self.random_rotation:\n",
    "                rand_angle = (randint(-10,10))\n",
    "                image = rotate(image, rand_angle, resize=True)\n",
    "                label = rotate(label, rand_angle, resize=True)\n",
    "                image = resize(image, (32,32,32))  #เนื่องจากพอ rotate แล้ว shape จะเปลี่ยน\n",
    "                label = resize(label, (32,32,32))\n",
    "            if self.get_path:\n",
    "                return torch.from_numpy(image).float().unsqueeze(0), torch.from_numpy(label).float().unsqueeze(0), img_path\n",
    "            return torch.from_numpy(image).float().unsqueeze(0), torch.from_numpy(label).float().unsqueeze(0)\n",
    "                                    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = DataLoaderImg(folder_path = \"test5\", random_rotation = False)\n",
    "valid_dataset = DataLoaderImg(folder_path = \"valid5\", random_rotation = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=num_workers)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (i,l) in train_loader:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing data with labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_marked(display_label):\n",
    "    layer_i = []\n",
    "    for i in range(display_label.shape[0]):\n",
    "        test_mask = display_label[i, :, :]\n",
    "        if np.sum(test_mask)>0:\n",
    "            layer_i.append(i)\n",
    "    if len(layer_i)>0:\n",
    "        return layer_i[int(len(layer_i)/2)]\n",
    "    else:\n",
    "        return int(display_label.shape[0]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_ct_scan_with_labels(loader, plot_size=50, cmap=plt.cm.gray):\n",
    "    \"\"\"accepts train_loader\"\"\"\n",
    "    data = next(iter(loader))\n",
    "    paths = data[2]\n",
    "    display_labels = data[1].squeeze().detach().cpu().numpy()\n",
    "    display_images = data[0].squeeze().detach().cpu().numpy() #1 batch of display_image 32,32,32,32\n",
    "    f, plots = plt.subplots(int(display_images[0].shape[0] / 4) , 4, figsize=(plot_size, plot_size))\n",
    "    f.suptitle('Label', fontsize=50, y=0.92)\n",
    "    for img in range(0, display_images.shape[0]): #batch_size\n",
    "        each_path = paths[img]\n",
    "        each_label = display_labels[img]\n",
    "        each_image = display_images[img]\n",
    "        marked = find_marked(each_label)\n",
    "        print(each_path)\n",
    "        plots[int((img / 4)), int(img % 4)].imshow(each_image[marked,:,:], cmap=\"gray\")\n",
    "        label =  np.ma.masked_where((each_label < 0.05), each_label)\n",
    "        plots[int((img / 4)), int(img % 4)].imshow(label[marked, :, :],cmap=\"hsv\", alpha=0.25) \n",
    "        plots[int((img / 4)), int(img % 4)].axis('off')\n",
    "        plots[int((img / 4)), int(img % 4)].set_title(str(each_path))        \n",
    "\n",
    "if False:\n",
    "    display_dataset = DataLoaderImg(folder_path = \"train5\",random_rotation = True, get_path=True)\n",
    "    display_loader = DataLoader(display_dataset, batch_size = 32, shuffle=True, num_workers=0)\n",
    "    plot_ct_scan_with_labels (display_loader)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_dataset = DataLoaderImg(folder_path = \"false_cand\",random_rotation = True, get_path=True)\n",
    "# display_loader = DataLoader(display_dataset, batch_size = 32, shuffle=True, num_workers=0)\n",
    "# data = next(iter(display_loader))\n",
    "# index = 0 #choose 0-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data[2][index])\n",
    "# display_label = data[1][index].squeeze().detach().cpu().numpy()\n",
    "# display_image = data[0][index].squeeze().detach().cpu().numpy() #1 batch\n",
    "\n",
    "# def explore_3dimage(layer=find_marked(display_label)):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(display_image[layer, :, :], cmap='gray')\n",
    "#     label =  np.ma.masked_where((display_label < 0.05), display_label)\n",
    "#     plt.imshow(label[layer, :, :], cmap=\"hsv\", alpha=0.1);  #label อยู่ตรงนี้นะ\n",
    "#     plt.title('Label', fontsize=20)\n",
    "#     plt.axis('off')\n",
    "#     return layer\n",
    "\n",
    "# interact(explore_3dimage, layer=(0, display_image.shape[0]))\n",
    "    \n",
    "# index += 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my first project outside of a course\n",
    "This time I'll be implementing 3d-conv in pytorch using U-net-like architecture\n",
    "\n",
    "https://www.researchgate.net/figure/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al_fig2_323597886\n",
    "\n",
    "note to self: \n",
    "- If you didn't add super init -> will cause error model called before init (when defining LunaModel class)! (it creates a proxy for that subclass?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_available : True\n",
      "GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if train_on_gpu else \"cpu\")\n",
    "print('GPU_available :',train_on_gpu)\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaBlockDown(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels):\n",
    "        super(LunaBlockDown, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(2,2)\n",
    "        self.batchnorm = nn.BatchNorm3d(conv_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        block_out = self.conv1(input_batch)\n",
    "        block_out = self.relu1(block_out)\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.batchnorm(block_out)\n",
    "        block_out = self.relu2(block_out)\n",
    "        \n",
    "        return self.maxpool(block_out), block_out     \n",
    "\n",
    "class LunaBlockUp(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels):\n",
    "        super(LunaBlockUp, self).__init__()\n",
    "        self.t_conv_layer = nn.ConvTranspose3d(\n",
    "            in_channels, in_channels,kernel_size =2, stride=2, padding=0, bias = False)\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.batchnorm = nn.BatchNorm3d(conv_channels)\n",
    "        \n",
    "# output size: (32-1)*s -2p+k = 64\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        block_out = self.t_conv_layer(input_batch)\n",
    "        block_out = self.conv1(block_out)\n",
    "        block_out = self.batchnorm(block_out)\n",
    "        block_out = self.relu1(block_out)\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.batchnorm(block_out)\n",
    "        block_out = self.relu2(block_out)\n",
    "        \n",
    "        return block_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LunaModel(\n",
       "  (block1): LunaBlockDown(\n",
       "    (conv1): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (batchnorm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (block2): LunaBlockDown(\n",
       "    (conv1): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (batchnorm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (block3): LunaBlockDown(\n",
       "    (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (block4): LunaBlockUp(\n",
       "    (t_conv_layer): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (block5): LunaBlockUp(\n",
       "    (t_conv_layer): ConvTranspose3d(256, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    (conv1): Conv3d(256, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (batchnorm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (block6): LunaBlockUp(\n",
       "    (t_conv_layer): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    (conv1): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (batchnorm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv1): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (conv3): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (batchnorm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LunaModel(nn.Module):\n",
    "    def __init__(self, in_channels=1, conv_channels=32):\n",
    "        super(LunaModel, self).__init__()\n",
    "        self.block1 = LunaBlockDown(in_channels, conv_channels) \n",
    "        self.block2 = LunaBlockDown(conv_channels, conv_channels * 2) \n",
    "        self.block3 = LunaBlockDown(conv_channels * 2,conv_channels * 4) \n",
    "        self.block4 = LunaBlockUp(conv_channels * 4, conv_channels * 4)\n",
    "        self.block5 = LunaBlockUp(conv_channels * 8, conv_channels*2)    \n",
    "        self.block6 = LunaBlockUp(conv_channels*4, conv_channels)  \n",
    "        self.conv1 = nn.Conv3d(\n",
    "            conv_channels*2, conv_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv3d(\n",
    "            conv_channels, in_channels, kernel_size=1, padding=0, bias=True)\n",
    "        self.batchnorm = nn.BatchNorm3d(conv_channels)\n",
    "        self.dropout = nn.Dropout(p=0.2, inplace=True)\n",
    "\n",
    "    \n",
    "    def forward(self, input_batch):\n",
    "        block_out, layer1 = self.block1(input_batch) #128x128x20\n",
    "        block_out, layer2 = self.block2(block_out) #64x64x10\n",
    "        block_out, layer3 = self.block3(block_out) #32x32x5\n",
    "        block_out = self.block4(block_out)\n",
    "        block_out = torch.cat((block_out, layer3), dim=1)\n",
    "        block_out = self.dropout(block_out)\n",
    "        block_out = self.block5(block_out)\n",
    "        block_out = torch.cat((block_out, layer2), dim=1)\n",
    "        block_out = self.dropout(block_out)\n",
    "        block_out = self.block6(block_out)\n",
    "        block_out = torch.cat((block_out, layer1), dim=1)\n",
    "        block_out = self.dropout(block_out)\n",
    "        block_out = self.conv1(block_out)\n",
    "        block_out = self.batchnorm(block_out)\n",
    "        block_out = self.relu1(block_out)\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.batchnorm(block_out)\n",
    "        block_out = self.relu2(block_out)\n",
    "        block_out = self.conv3(block_out)\n",
    "#         no batchnorm for last layer\n",
    "        \n",
    "#         return torch.sigmoid(block_out)    #if use dice_loss\n",
    "        return block_out     #if use BCEWithLogitsLoss\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) in {\n",
    "                nn.Conv3d,\n",
    "                nn.ConvTranspose3d\n",
    "            }:nn.init.kaiming_normal_(\n",
    "                m.weight.data, a=0, mode='fan_out', nonlinearity='relu', \n",
    "            ) \n",
    "            if m.bias is not None:\n",
    "                fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(m.weight.data) \n",
    "                bound = 1 / math.sqrt(fan_out) \n",
    "                nn.init.normal_(m.bias, -bound, bound)\n",
    "\n",
    "                \n",
    "model = LunaModel()\n",
    "# move everything to gpu\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3305217\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft dice loss https://www.jeremyjordan.me/semantic-segmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Note: soft dice loss not dice loss\n",
    "class diceloss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Compute mean dice coefficient over all abnormality classes.\n",
    "\n",
    "    Args:\n",
    "        y_true (tensor): shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        y_pred (tensor): tensor of predictions for all classes.\n",
    "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        axis (tuple): spatial axes to sum over when computing numerator and\n",
    "                      denominator of dice coefficient.\n",
    "                      Hint: pass this as the 'axis' argument to the K.sum\n",
    "                            and K.mean functions.\n",
    "        epsilon (float): small constant add to numerator and denominator to\n",
    "                        avoid divide by 0 errors.\n",
    "    Returns:\n",
    "        dice_coefficient (float): computed value of dice coefficient.     \n",
    "    \"\"\"\n",
    "    def init(self):\n",
    "        super(diceLoss, self).init()\n",
    "    def forward(self,y_true, y_pred, epsilon=0.00001):\n",
    "        axis = tuple(range(1, len(y_pred.shape)-1)) \n",
    "        dice_numerator = torch.sum(y_pred*y_true,axis=axis)*2 + epsilon\n",
    "        dice_denominator = torch.sum(y_true**2,axis=axis) + torch.sum(y_pred**2,axis=axis) + epsilon\n",
    "        dice_loss = 1-torch.mean(dice_numerator/dice_denominator)\n",
    "        return dice_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix\n",
    "def compute_CM (pred, label, threshold=0.5):\n",
    "    \"\"\"Only accepts numpy..\"\"\"\n",
    "    threshold = 0.5\n",
    "    pred[pred >= threshold] = 1.0\n",
    "    pred[pred < threshold] = 0.0\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    label = label.detach().cpu().numpy()\n",
    "    tp = np.sum((pred == 1) & (label==1))\n",
    "    tn = np.sum((pred == 0) & (label==0))\n",
    "    fp = np.sum((pred == 1) & (label==0))\n",
    "    fn = np.sum((pred == 0) & (label==1))\n",
    "    accuracy = ((tp+tn)/(tp+tn+fp+fn))\n",
    "    sensitivity = ((tp/(tp+fn)))\n",
    "    specificity = (tn/(tn+fp))\n",
    "    ppv = (tp/(tp+fp))\n",
    "    return round(accuracy*100, 2),round(sensitivity*100, 2),round(specificity*100, 2),round(ppv*100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectified Adam https://www.kaggle.com/dhananjay3/image-segmentation-from-scratch-in-pytorch\n",
    "# RAdam better than Adam? https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b\n",
    "# note to self learning rate decay of 0.94 is too fast since my dataset is small (10 epochs = 0.50) -> adjusted to 0.972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "lr         = 1*1e-3\n",
    "\n",
    "use_learning_rate_decay = True #set to false when use LR finder\n",
    "if use_learning_rate_decay == True:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    LRdecayRate = 0.972\n",
    "    my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=LRdecayRate)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "        \n",
    "# criterion = diceloss()\n",
    "pos_weight = torch.ones([1]).to(device)*70 # 1 class and weight = 70, the number of neg_px is about 70 times that of pos_px in my training dataset\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal learning rate with : https://github.com/davidtvs/pytorch-lr-finder\n",
    "*cylical learning rate vs adaptive learning rate?*\n",
    "\n",
    "cyclical -> believes difficulty in minimizing the loss arises from \"saddle points\" rather than poor local minima.\n",
    "Cyclical Learning Rates for Training Neural Networks by Leslie N. Smith https://arxiv.org/abs/1506.01186\n",
    "\n",
    "The code from the link used cyclical learning rate to find optimal lr...\n",
    "while I will just use simple leanring rate decay.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_learning_rate_decay == False:\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch_lr_finder import LRFinder\n",
    "\n",
    "    model.load_state_dict(torch.load('save/save43.pt'))\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(valid_loader, end_lr=5*1e-5, num_iter=200, step_mode=\"exp\")\n",
    "    lr_finder.plot()\n",
    "    lr_finder.reset()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal learning rate \n",
    "The optimal learning rate from code above is about 5*1e-4\n",
    "![Optimal learning rate](model4lr.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_send_line(text):\n",
    "    try:\n",
    "        line.sendtext(str(text))\n",
    "    except:\n",
    "        pass\n",
    "    print(str(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new training session\n",
      "Epoch: 1/150, train_loss: 56.101011753082275\n",
      "--- 14.523430585861206 seconds ---\n",
      "Epoch: 1/150, train_loss: 19.294410973787308\n",
      "--- 27.851077556610107 seconds ---\n",
      "Epoch: 1/150, train_loss: 0.7109025748463782, Validation_loss: 1.359023 \n",
      "Accuracy: 99.67%, Sensitivity: 39.09%, Specificity: 99.99%, , PPV: 96.12%\n",
      "--- 42.92162847518921 seconds ---\n",
      "val loss min decreased, Saving model...61\n",
      "Epoch: 2/150, train_loss: 16.00044561177492\n",
      "--- 13.394595623016357 seconds ---\n",
      "Epoch: 2/150, train_loss: 6.078459993004799\n",
      "--- 26.784590005874634 seconds ---\n",
      "Epoch: 2/150, train_loss: 0.18504487616997778, Validation_loss: 1.182263 \n",
      "Accuracy: 98.87%, Sensitivity: 0.0%, Specificity: 99.91%, , PPV: 0.0%\n",
      "--- 41.83538341522217 seconds ---\n",
      "val loss min decreased, Saving model...62\n",
      "Epoch: 3/150, train_loss: 5.974815353751183\n",
      "--- 13.397319793701172 seconds ---\n",
      "Epoch: 3/150, train_loss: 8.684455767273903\n",
      "--- 26.815130472183228 seconds ---\n",
      "Epoch: 3/150, train_loss: 0.12205749487673695, Validation_loss: 1.620313 \n",
      "Accuracy: 99.79%, Sensitivity: 57.66%, Specificity: 99.88%, , PPV: 51.35%\n",
      "--- 41.926544427871704 seconds ---\n",
      "Epoch: 4/150, train_loss: 5.957275032997131\n",
      "--- 13.404471635818481 seconds ---\n",
      "Epoch: 4/150, train_loss: 6.568534268066287\n",
      "--- 26.799806833267212 seconds ---\n",
      "Epoch: 4/150, train_loss: 0.09661072442505622, Validation_loss: 1.681181 \n",
      "Accuracy: 99.41%, Sensitivity: 1.47%, Specificity: 99.9%, , PPV: 7.2%\n",
      "--- 41.89630722999573 seconds ---\n",
      "Epoch: 5/150, train_loss: 6.829021938145161\n",
      "--- 13.476001501083374 seconds ---\n",
      "Epoch: 5/150, train_loss: 4.521480277180672\n",
      "--- 27.004320859909058 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  \n",
      "C:\\Users\\Mark\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/150, train_loss: 0.08618957686491988, Validation_loss: 1.676223 \n",
      "Accuracy: 100.0%, Sensitivity: nan%, Specificity: 100.0%, , PPV: nan%\n",
      "--- 42.14767909049988 seconds ---\n",
      "Epoch: 6/150, train_loss: 4.149849571287632\n",
      "--- 13.699477195739746 seconds ---\n",
      "Epoch: 6/150, train_loss: 2.9803215451538563\n",
      "--- 27.190603256225586 seconds ---\n",
      "Epoch: 6/150, train_loss: 0.06651183249936862, Validation_loss: 2.215629 \n",
      "Accuracy: 96.9%, Sensitivity: 31.44%, Specificity: 99.98%, , PPV: 98.72%\n",
      "--- 42.37597608566284 seconds ---\n",
      "Epoch: 7/150, train_loss: 5.058951169252396\n",
      "--- 13.49903130531311 seconds ---\n",
      "Epoch: 7/150, train_loss: 5.3347784876823425\n",
      "--- 26.969959259033203 seconds ---\n",
      "Epoch: 7/150, train_loss: 0.07555288080422377, Validation_loss: 2.395963 \n",
      "Accuracy: 99.15%, Sensitivity: 48.86%, Specificity: 99.98%, , PPV: 96.98%\n",
      "--- 42.1494677066803 seconds ---\n",
      "sum of params in confusion matrix increased, Saving model...63\n",
      "Epoch: 8/150, train_loss: 5.453981753438711\n",
      "--- 13.490033864974976 seconds ---\n",
      "Epoch: 8/150, train_loss: 3.1370370388031006\n",
      "--- 26.94985270500183 seconds ---\n",
      "Epoch: 8/150, train_loss: 0.06876000161635491, Validation_loss: 2.402126 \n",
      "Accuracy: 99.61%, Sensitivity: 31.8%, Specificity: 100.0%, , PPV: 99.04%\n",
      "--- 42.13442254066467 seconds ---\n",
      "Epoch: 9/150, train_loss: 4.098452717065811\n",
      "--- 13.483704566955566 seconds ---\n",
      "Epoch: 9/150, train_loss: 3.1913172528147697\n",
      "--- 26.919642448425293 seconds ---\n",
      "Epoch: 9/150, train_loss: 0.06684411767249306, Validation_loss: 1.548794 \n",
      "Accuracy: 99.77%, Sensitivity: 64.63%, Specificity: 99.91%, , PPV: 76.13%\n",
      "--- 42.05960988998413 seconds ---\n",
      "Epoch: 10/150, train_loss: 4.370036073029041\n",
      "--- 13.479957580566406 seconds ---\n",
      "Epoch: 10/150, train_loss: 3.5973080918192863\n",
      "--- 26.977208852767944 seconds ---\n",
      "Epoch: 10/150, train_loss: 0.07199155664261939, Validation_loss: 1.565343 \n",
      "Accuracy: 97.95%, Sensitivity: 53.88%, Specificity: 99.7%, , PPV: 87.93%\n",
      "--- 42.10688138008118 seconds ---\n",
      "Epoch: 11/150, train_loss: 4.915616527199745\n",
      "--- 13.485382080078125 seconds ---\n",
      "Epoch: 11/150, train_loss: 6.752983413636684\n",
      "--- 26.974523782730103 seconds ---\n",
      "Epoch: 11/150, train_loss: 0.09557136204093694, Validation_loss: 2.474930 \n",
      "Accuracy: 99.83%, Sensitivity: 24.17%, Specificity: 99.99%, , PPV: 90.44%\n",
      "--- 42.19849443435669 seconds ---\n",
      "Epoch: 12/150, train_loss: 4.119755439460278\n",
      "--- 13.452775239944458 seconds ---\n",
      "Epoch: 12/150, train_loss: 5.909391097724438\n",
      "--- 26.89939832687378 seconds ---\n",
      "Epoch: 12/150, train_loss: 0.0677879754002347, Validation_loss: 3.065183 \n",
      "Accuracy: 99.52%, Sensitivity: 18.96%, Specificity: 100.0%, , PPV: 99.22%\n",
      "--- 42.07421159744263 seconds ---\n",
      "Epoch: 13/150, train_loss: 3.141702465713024\n",
      "--- 13.46562910079956 seconds ---\n",
      "Epoch: 13/150, train_loss: 4.163700572215021\n",
      "--- 26.97159242630005 seconds ---\n",
      "Epoch: 13/150, train_loss: 0.06207758234954919, Validation_loss: 2.685503 \n",
      "Accuracy: 99.81%, Sensitivity: 36.15%, Specificity: 100.0%, , PPV: 100.0%\n",
      "--- 42.15448880195618 seconds ---\n",
      "Epoch: 14/150, train_loss: 5.641276337206364\n",
      "--- 13.464693546295166 seconds ---\n",
      "Epoch: 14/150, train_loss: 4.290217101573944\n",
      "--- 26.936278343200684 seconds ---\n",
      "Epoch: 14/150, train_loss: 0.07416969301941043, Validation_loss: 2.519411 \n",
      "Accuracy: 99.34%, Sensitivity: 6.95%, Specificity: 99.98%, , PPV: 71.9%\n",
      "--- 42.1257688999176 seconds ---\n",
      "Epoch: 15/150, train_loss: 3.4925575498491526\n",
      "--- 13.511017560958862 seconds ---\n",
      "Epoch: 15/150, train_loss: 3.791438691318035\n",
      "--- 26.98923397064209 seconds ---\n",
      "Epoch: 15/150, train_loss: 0.05536479211033579, Validation_loss: 2.772397 \n",
      "Accuracy: 98.52%, Sensitivity: 46.69%, Specificity: 100.0%, , PPV: 99.97%\n",
      "--- 42.166067123413086 seconds ---\n",
      "sum of params in confusion matrix increased, Saving model...64\n",
      "Epoch: 16/150, train_loss: 3.658613756299019\n",
      "--- 13.461984872817993 seconds ---\n",
      "Epoch: 16/150, train_loss: 3.586593007668853\n",
      "--- 26.922121047973633 seconds ---\n",
      "Epoch: 16/150, train_loss: 0.05727911710344029, Validation_loss: 2.833866 \n",
      "Accuracy: 97.7%, Sensitivity: 21.9%, Specificity: 99.98%, , PPV: 96.5%\n",
      "--- 42.085816621780396 seconds ---\n",
      "Epoch: 17/150, train_loss: 3.3065711315721273\n",
      "--- 13.484837293624878 seconds ---\n",
      "Epoch: 17/150, train_loss: 2.394871886819601\n",
      "--- 26.990054845809937 seconds ---\n",
      "Epoch: 17/150, train_loss: 0.04670547143623205, Validation_loss: 3.921273 \n",
      "Accuracy: 98.14%, Sensitivity: 18.27%, Specificity: 100.0%, , PPV: 100.0%\n",
      "--- 42.24014067649841 seconds ---\n",
      "Epoch: 18/150, train_loss: 2.5915853939950466\n",
      "--- 13.502268075942993 seconds ---\n",
      "Epoch: 18/150, train_loss: 2.623451419174671\n",
      "--- 26.992416381835938 seconds ---\n",
      "Epoch: 18/150, train_loss: 0.04541602777052586, Validation_loss: 3.655726 \n",
      "Accuracy: 100.0%, Sensitivity: nan%, Specificity: 100.0%, , PPV: nan%\n",
      "--- 42.16069030761719 seconds ---\n",
      "Epoch: 19/150, train_loss: 2.2051602490246296\n",
      "--- 13.487030982971191 seconds ---\n",
      "Epoch: 19/150, train_loss: 3.1343319714069366\n",
      "--- 26.975746631622314 seconds ---\n",
      "Epoch: 19/150, train_loss: 0.04219460863986928, Validation_loss: 4.566634 \n",
      "Accuracy: 99.7%, Sensitivity: 0.58%, Specificity: 100.0%, , PPV: 36.36%\n",
      "--- 42.253127098083496 seconds ---\n",
      "Epoch: 20/150, train_loss: 2.214533777208999\n",
      "--- 13.498028755187988 seconds ---\n",
      "Epoch: 20/150, train_loss: 2.5745261665433645\n",
      "--- 26.96059799194336 seconds ---\n",
      "Epoch: 20/150, train_loss: 0.036791472708467735, Validation_loss: 4.283520 \n",
      "Accuracy: 95.97%, Sensitivity: 2.13%, Specificity: 100.0%, , PPV: 100.0%\n",
      "--- 42.13783264160156 seconds ---\n",
      "Epoch: 21/150, train_loss: 1.8988377582281828\n",
      "--- 13.462938070297241 seconds ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-22992e19f255>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#         apply gradient clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtrain_loss_small\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('save/save60.pt'))\n",
    "print_and_send_line(\"Starting new training session\")\n",
    "\n",
    "losses = []\n",
    "\n",
    "# valid_loss_min = np.Inf\n",
    "valid_loss_min = 6  #for BCE loss(w=70)\n",
    "# valid_loss_min = 2.3 #for Dice loss\n",
    "CM_min = 141 #confusion matrix\n",
    "print_every = 15\n",
    "clip = 1\n",
    "save_number = 1\n",
    "total_time = time.time()\n",
    "for e in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_loss_small = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ################### # train the model # ###################\n",
    "    model.train() \n",
    "    for batch_i,(i, l) in enumerate(train_loader,1):\n",
    "        optimizer.zero_grad()\n",
    "        i, l = i.to(device), l.to(device)\n",
    "        pred = model(i)\n",
    "        loss = criterion(pred,l)\n",
    "        loss.backward()\n",
    "#         apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = clip)\n",
    "        optimizer.step()\n",
    "        train_loss_small += loss.item()*i.size(0)\n",
    "        train_loss += train_loss_small\n",
    "        if batch_i%print_every == 0:\n",
    "            print(\"Epoch: {}/{}, train_loss: {}\".format(e+1,num_epochs, train_loss_small))\n",
    "            try:\n",
    "                line.sendtext(\"Epoch: {}/{}, train_loss: {}\".format(e+1,num_epochs, train_loss_small))\n",
    "            except:\n",
    "                pass\n",
    "            train_loss_small = 0.0\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "    ###################### # validate the model # ###################### \n",
    "    model.eval()\n",
    "    for (i,l) in valid_loader:\n",
    "        i, l = i.to(device), l.to(device)\n",
    "        pred = model(i)\n",
    "        loss = criterion(pred,l)\n",
    "        valid_loss += loss.item()*i.size(0)   \n",
    "    \n",
    "    my_lr_scheduler.step()\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    losses.append((train_loss, valid_loss))    \n",
    "    accuracy, sensitivity, specificity, ppv = compute_CM(pred,l)\n",
    "    \n",
    "    print_and_send_line(\"Epoch: {}/{}, train_loss: {}, Validation_loss: {:.6f} \".format(e+1,num_epochs, train_loss, valid_loss))\n",
    "    print_and_send_line(\"Accuracy: {}%, Sensitivity: {}%, Specificity: {}%, , PPV: {}%\".format(accuracy, sensitivity, specificity, ppv))\n",
    "    print_and_send_line(\"--- %s seconds ---\" % (time.time() - start_time))   \n",
    "    \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        while os.path.isfile('save/save{}.pt'.format(save_number)):\n",
    "            save_number += 1\n",
    "        print_and_send_line (\"val loss min decreased, Saving model...{}\".format(save_number))\n",
    "        torch.save(model.state_dict(), 'save/save{}.pt'.format(save_number))\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "    CM =  sensitivity + ppv\n",
    "    if CM > CM_min:\n",
    "        while os.path.isfile('save/save{}.pt'.format(save_number)):\n",
    "            save_number += 1\n",
    "        print_and_send_line (\"sum of params in confusion matrix increased, Saving model...{}\".format(save_number))\n",
    "        torch.save(model.state_dict(), 'save/save{}.pt'.format(save_number))\n",
    "        CM_min = CM\n",
    "\n",
    "back_up = \"save/\" + datetime.datetime.now().strftime(\"%d-%m-%Y_%H-%M\") + \".pt\"\n",
    "torch.save(model.state_dict(), back_up)\n",
    "print_and_send_line(\"Finished Training, Total time elapsed: \".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'save/save39.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualing Results\n",
    "\n",
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Train_loss', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Valid_loss', alpha=0.5)\n",
    "plt.title(\"Losses\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds in comparison to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('save/save19.pt'))\n",
    "# display_dataset = DataLoaderImg(folder_path = \"test5\",random_rotation = True, get_path=True)\n",
    "display_dataset = DataLoaderImg(folder_path = \"test5\",random_rotation = True, get_path=True)\n",
    "display_loader = DataLoader(display_dataset, batch_size = 50, shuffle=True, num_workers=0)\n",
    "model.eval()\n",
    "def plot_ct_scan_with_labels_and_pred(loader, plot_size=50, cmap=plt.cm.gray):\n",
    "    \"\"\"accepts train_loader\"\"\"\n",
    "    data = next(iter(loader))\n",
    "    i = data[0].to(device)\n",
    "    pred = model(i).squeeze().detach().cpu().numpy()\n",
    "    display_labels = data[1].squeeze().detach().cpu().numpy()\n",
    "    display_images = i.squeeze().detach().cpu().numpy() #1 batch of display_image 32,32,32,32\n",
    "    paths = data[2]\n",
    "\n",
    "    f, plots = plt.subplots(int(display_images.shape[0] / 2) , 4, figsize=(plot_size, plot_size))\n",
    "    f.suptitle('Red = Label, Yellow = Prediction', fontsize=50, y=0.92, x=0.2)\n",
    "    for img in range(0, display_images.shape[0]): #batch_size\n",
    "        each_path = paths[img]\n",
    "        each_label = display_labels[img]\n",
    "        each_image = display_images[img]\n",
    "        each_pred = pred[img]\n",
    "        marked = find_marked(each_label)\n",
    "        print(each_path)\n",
    "        \n",
    "        plots[int((img / 2)), int(img % 2)*2].imshow(each_image[marked,:,:], cmap=\"gray\")\n",
    "        plots[int((img / 2)), (int(img % 2)*2)+1].imshow(each_image[marked,:,:], cmap=\"gray\")\n",
    "        label =  np.ma.masked_where((each_label < 0.05), each_label)\n",
    "        pred_mask = np.ma.masked_where((each_pred < 0.05), each_pred)\n",
    "        plots[int((img / 2)), int(img % 2)*2].imshow(label[marked, :, :],cmap=\"hsv\", alpha=0.5) \n",
    "        plots[int((img / 2)), (int(img % 2)*2)+1].imshow(pred_mask[marked,:,:], cmap=\"Wistia\", alpha=1.0)\n",
    "        plots[int((img / 2)), int(img % 2)*2].axis('off')\n",
    "        plots[int((img / 2)), (int(img % 2)*2)+1].axis('off')\n",
    "        plots[int((img / 2)), int(img % 2)*2].set_title(str(each_path))        \n",
    "        plt.subplots_adjust(wspace=0, hspace=0.2, left=0, right=0.4)\n",
    "\n",
    "\n",
    "plot_ct_scan_with_labels_and_pred (display_loader)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 31 #ใส่เลข 0-31\n",
    "\n",
    "display_pred = pred[index].squeeze().detach().cpu().numpy()\n",
    "display_label = l[index].squeeze().detach().cpu().numpy()\n",
    "display_image = i[index].squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_3dimage(layer=find_marked(display_label)):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(display_image[layer, :, :], cmap='gray');\n",
    "    global display_pred\n",
    "    mask = np.ma.masked_where((display_pred < 0.05), display_pred)\n",
    "    plt.imshow(mask[layer, :, :], cmap=\"Wistia\", alpha=0.5);   #mask อยู่ตรงนี้นะ\n",
    "    label =  np.ma.masked_where((display_label < 0.05), display_label)\n",
    "    plt.imshow(label[layer, :, :], cmap=\"hsv\", alpha=0.5);  #label อยู่ตรงนี้นะ\n",
    "    plt.title('Label', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    return layer\n",
    "\n",
    "interact(explore_3dimage, layer=(0, display_image.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find false positive candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the file containing the false positive candidates and move it to another location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_false_cands(loader, plot_size=50, cmap=plt.cm.gray):\n",
    "    \"\"\"accepts train_loader\"\"\"\n",
    "    data = next(iter(loader))\n",
    "    i = data[0].to(device)\n",
    "    pred = model(i).squeeze().detach().cpu().numpy()\n",
    "    display_labels = data[1].squeeze().detach().cpu().numpy()\n",
    "    display_images = i.squeeze().detach().cpu().numpy() #1 batch of display_image 32,32,32,32\n",
    "    paths = data[2]\n",
    "    all_paths = []\n",
    "    for img in range(0, display_images.shape[0]): #batch_size\n",
    "        each_path = paths[img]\n",
    "        each_label = display_labels[img]\n",
    "        each_image = display_images[img]\n",
    "        each_pred = pred[img]\n",
    "        marked = find_marked(each_pred)\n",
    "\n",
    "        all_paths.append(each_path) #will delete files that are not moved\n",
    "        for x in range(each_pred.shape[0]):\n",
    "            test_mask = each_pred[x, :, :]\n",
    "#             print(np.sum(test_mask))\n",
    "            if np.sum(test_mask)>-15500:\n",
    "                try:\n",
    "                    shutil.move(each_path, 'train5-2')\n",
    "                except:\n",
    "                    pass\n",
    "    for y in all_paths:\n",
    "        try:\n",
    "            os.remove(y)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "# for i in range (0,1000):\n",
    "#     model.load_state_dict(torch.load('save/save22.pt'))\n",
    "#     display_dataset = DataLoaderImg(folder_path = \"false_cand\",random_rotation = True, get_path=True)\n",
    "#     display_loader = DataLoader(display_dataset, batch_size = 50, shuffle=True, num_workers=0)\n",
    "#     model.eval()\n",
    "#     all_paths = find_false_cands (display_loader)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
